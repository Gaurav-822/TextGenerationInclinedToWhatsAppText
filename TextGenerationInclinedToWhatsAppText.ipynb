{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1rIU4_GzAfHuqQVr0TVdDCf0BMkv1xkX3",
      "authorship_tag": "ABX9TyP3uCrY0NL50wzuRE+M95DZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaurav-822/TextGenerationInclinedToWhatsAppText/blob/main/TextGenerationInclinedToWhatsAppText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installing Neccessary Packages"
      ],
      "metadata": {
        "id": "Y2-lXwXVeS0i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oExHKpsd2TUP"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Pre-trained Model google/flan-t5-base (~1GB)"
      ],
      "metadata": {
        "id": "BBDZpfYEeat6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "ToqjXegtEzmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tuning the Model\n",
        "Fine tune the model using Whatsapp imported text after processing the imported text"
      ],
      "metadata": {
        "id": "iJaD9Pmyer6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the whatsapp imported text and rename it to chat.txt then run this cell"
      ],
      "metadata": {
        "id": "xTO-cTEFfQXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to preprocess the imported text\n",
        "import re\n",
        "\n",
        "# Using Regex to Clean the date and time stamps\n",
        "def clear_time_stamps(text):\n",
        "  pattern = r\"\\b\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2}â€¯[APap][Mm] - \\b\"\n",
        "  return re.sub(pattern, \"\", text)\n",
        "\n",
        "file_path = 'chat.txt'\n",
        "txt = \"\"\n",
        "\n",
        "# Open the file in read mode ('r')\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        txt += clear_time_stamps(line)\n",
        "\n",
        "print(txt)\n",
        "\n",
        "file_path = 'cleaned_chat1.txt'\n",
        "\n",
        "# Open the file in write mode ('w')\n",
        "with open(file_path, 'w') as file:\n",
        "  file.write(txt)"
      ],
      "metadata": {
        "id": "IbkUT2M8fAbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the name of \"Gaurav\" and \"Anaysha\" with character 1 and character 2 names in the loop of this cell first"
      ],
      "metadata": {
        "id": "p0yMA6PIfkYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with the prepared chats\n",
        "train_data = [\n",
        "    (\"Hello Anaysha!\", \"Hi!\"),\n",
        "    # Adding our chats in this list as a form of tuple\n",
        "]\n",
        "\n",
        "# Adding elements to train_data\n",
        "temp = []\n",
        "ip = \"\"\n",
        "op = \"\"\n",
        "c = 1\n",
        "with open(\"cleaned_chat1.txt\", 'r') as file:\n",
        "    for line in file:\n",
        "      if len(temp) == 2:\n",
        "        train_data.append(tuple(temp))\n",
        "        temp = []\n",
        "      elif \"Gaurav\" in line:  # change \"Gaurav\" with character 1\n",
        "          ip += (line + ' ')\n",
        "          c = 0\n",
        "      elif \"Anaysha\" in line: # change \"Anaysha\" with character 2\n",
        "          op += (line + ' ')\n",
        "          c = 1\n",
        "      else:\n",
        "          if c == 0:\n",
        "            ip += (line + ' ')\n",
        "          else:\n",
        "            op += (line + ' ')\n",
        "      if ip != \"\" and op != \"\":\n",
        "        temp = [ip, op]\n",
        "        ip = \"\"\n",
        "        op = \"\"\n",
        "        train_data.append(tuple(temp))\n",
        "        temp = []\n",
        "\n",
        "# Tokenize and prepare the training data\n",
        "train_encodings = tokenizer.prepare_seq2seq_batch(\n",
        "    src_texts=[item[0] for item in train_data],\n",
        "    tgt_texts=[item[1] for item in train_data],\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"tf\"\n",
        ")\n",
        "\n",
        "# Extract input and output tensors\n",
        "input_ids = train_encodings[\"input_ids\"]\n",
        "attention_mask = train_encodings[\"attention_mask\"]\n",
        "labels = train_encodings[\"labels\"]\n",
        "\n",
        "# Compile and train the model\n",
        "optimizer = Adam(learning_rate=3e-5)\n",
        "model.compile(optimizer=optimizer)\n",
        "model.fit(\n",
        "    {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
        "    {\"labels\": labels},\n",
        "    epochs=1,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "# Test the fine-tuned model\n",
        "input_text = \"Do you love me?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"tf\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "QpNgKXJ3E4Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate text with the trained model\n",
        "Results will be inclined to the training chats provided"
      ],
      "metadata": {
        "id": "z2VVCnzzgYev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fine-tuned model\n",
        "input_text = \"I love you <3\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"tf\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "# print(outputs)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "b86ccOyaBZh8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}